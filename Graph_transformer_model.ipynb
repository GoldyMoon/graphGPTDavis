{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Assuming G is your networkx graph object from before\n",
    "\n",
    "# Nodes\n",
    "nodes = list(G_clean.nodes())\n",
    "\n",
    "# Edges\n",
    "edges = list(G_clean.edges())\n",
    "\n",
    "# Node Attributes\n",
    "# This is a placeholder. You'll need to define attributes based on your graph's specifics.\n",
    "node_attributes = {node:  G_clean.nodes[node] for node in  G_clean.nodes()}\n",
    "\n",
    "# Adjacency Matrix\n",
    "adj_matrix = nx.to_numpy_matrix( G_clean, nodelist=nodes)\n",
    "\n",
    "# Creating the tuple\n",
    "graph_tuple = (nodes, edges, node_attributes, adj_matrix)\n",
    "\n",
    "# Now, graph_tuple contains your graph's representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "'CWE-319'\n",
      "'CWE-6'\n",
      "'CWE-756'\n",
      "'CWE-668'\n",
      "'CWE-266'\n",
      "(\"'CWE-319'\", \"'CWE-311'\")\n",
      "(1497, 1497)\n"
     ]
    }
   ],
   "source": [
    "def check_if_all_zeros(adj_matrix):\n",
    "    return np.all(adj_matrix == 0)\n",
    "\n",
    "is_all_zero_example = check_if_all_zeros(graph_tuple[3][0])\n",
    "print(is_all_zero_example)\n",
    "# print(graph_tuple[0])\n",
    "# print(graph_tuple[1])\n",
    "# print(graph_tuple[2])\n",
    "print(graph_tuple[0][0])\n",
    "print(graph_tuple[0][1])\n",
    "print(graph_tuple[0][2])\n",
    "print(graph_tuple[0][3])\n",
    "print(graph_tuple[0][4])\n",
    "print(graph_tuple[1][0])\n",
    "\n",
    "print(graph_tuple[3].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the graph G (with out description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_graph_from_file(file_path):\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            elements = line.strip().split(', ')\n",
    "            if len(elements) == 3:\n",
    "                source, relation, target = elements\n",
    "                G.add_node(source)\n",
    "                G.add_node(target)\n",
    "                G.add_edge(source, target, relation=relation)\n",
    "    \n",
    "    return G\n",
    "\n",
    "def draw_graph(G):\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw_networkx_nodes(G, pos)\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "    nx.draw_networkx_labels(G, pos)\n",
    "    edge_labels = nx.get_edge_attributes(G, 'relation')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "file_path = 'kb.txt'\n",
    "G = build_graph_from_file(file_path)\n",
    "# draw_graph(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Assuming G is your existing networkx graph with nodes needing cleanup\n",
    "\n",
    "def clean_node_name(node):\n",
    "    return node.replace('[', '').replace(']', '')\n",
    "\n",
    "# Create a new graph to hold cleaned nodes and edges\n",
    "G_clean = nx.DiGraph()\n",
    "\n",
    "# Copy and clean each node and its attributes\n",
    "for node in G.nodes(data=True):\n",
    "    clean_node = clean_node_name(node[0])\n",
    "    G_clean.add_node(clean_node, **node[1])\n",
    "\n",
    "# Copy edges to the new graph with cleaned node names\n",
    "for edge in G.edges(data=True):\n",
    "    clean_source = clean_node_name(edge[0])\n",
    "    clean_target = clean_node_name(edge[1])\n",
    "    G_clean.add_edge(clean_source, clean_target, **edge[2])\n",
    "\n",
    "# Now G_clean contains your graph with cleaned node names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss: 0.6103078722953796\n",
      "Epoch 2: Loss: 1.298105001449585\n",
      "Epoch 3: Loss: 1.0220133066177368\n",
      "Epoch 4: Loss: 0.8595145344734192\n",
      "Epoch 5: Loss: 0.910021960735321\n",
      "Epoch 6: Loss: 0.67376708984375\n",
      "Epoch 7: Loss: 0.617800235748291\n",
      "Epoch 8: Loss: 0.6802899837493896\n",
      "Epoch 9: Loss: 0.6780117154121399\n",
      "Epoch 10: Loss: 0.6075509190559387\n",
      "Epoch 11: Loss: 0.6708061099052429\n",
      "Epoch 12: Loss: 0.6125102639198303\n",
      "Epoch 13: Loss: 0.685776948928833\n",
      "Epoch 14: Loss: 0.6087173223495483\n",
      "Epoch 15: Loss: 0.6294495463371277\n",
      "Epoch 16: Loss: 0.6278523802757263\n",
      "Epoch 17: Loss: 0.6665735840797424\n",
      "Epoch 18: Loss: 0.6652699708938599\n",
      "Epoch 19: Loss: 0.5956849455833435\n",
      "Epoch 20: Loss: 0.6626076698303223\n",
      "Epoch 21: Loss: 0.6830970644950867\n",
      "Epoch 22: Loss: 0.5846443772315979\n",
      "Epoch 23: Loss: 0.6842755675315857\n",
      "Epoch 24: Loss: 0.6803540587425232\n",
      "Epoch 25: Loss: 0.6117826104164124\n",
      "Epoch 26: Loss: 0.6555581092834473\n",
      "Epoch 27: Loss: 0.601810872554779\n",
      "Epoch 28: Loss: 0.6535786986351013\n",
      "Epoch 29: Loss: 0.6526003479957581\n",
      "Epoch 30: Loss: 0.651620090007782\n",
      "Epoch 31: Loss: 0.6178202629089355\n",
      "Epoch 32: Loss: 0.6497113108634949\n",
      "Epoch 33: Loss: 0.6153978109359741\n",
      "Epoch 34: Loss: 0.6479185223579407\n",
      "Epoch 35: Loss: 0.6470678448677063\n",
      "Epoch 36: Loss: 0.5985479354858398\n",
      "Epoch 37: Loss: 0.6454843878746033\n",
      "Epoch 38: Loss: 0.5824952721595764\n",
      "Epoch 39: Loss: 0.6440919041633606\n",
      "Epoch 40: Loss: 0.6434572339057922\n",
      "Epoch 41: Loss: 0.6765584945678711\n",
      "Epoch 42: Loss: 0.6423197984695435\n",
      "Epoch 43: Loss: 0.6418114304542542\n",
      "Epoch 44: Loss: 0.555095374584198\n",
      "Epoch 45: Loss: 0.6182520985603333\n",
      "Epoch 46: Loss: 0.6746573448181152\n",
      "Epoch 47: Loss: 0.6556893587112427\n",
      "Epoch 48: Loss: 0.5764055252075195\n",
      "Epoch 49: Loss: 0.5858572125434875\n",
      "Epoch 50: Loss: 0.6397126317024231\n",
      "Epoch 51: Loss: 0.5957930088043213\n",
      "Epoch 52: Loss: 0.5956212878227234\n",
      "Epoch 53: Loss: 0.5908724665641785\n",
      "Epoch 54: Loss: 0.6390369534492493\n",
      "Epoch 55: Loss: 0.6388874650001526\n",
      "Epoch 56: Loss: 0.6387343406677246\n",
      "Epoch 57: Loss: 0.5640090107917786\n",
      "Epoch 58: Loss: 0.5692086815834045\n",
      "Epoch 59: Loss: 0.6383748650550842\n",
      "Epoch 60: Loss: 0.6382834315299988\n",
      "Epoch 61: Loss: 0.5675768256187439\n",
      "Epoch 62: Loss: 0.6381264328956604\n",
      "Epoch 63: Loss: 0.5610406994819641\n",
      "Epoch 64: Loss: 0.6380134224891663\n",
      "Epoch 65: Loss: 0.5627855658531189\n",
      "Epoch 66: Loss: 0.5604166388511658\n",
      "Epoch 67: Loss: 0.63791424036026\n",
      "Epoch 68: Loss: 0.550973653793335\n",
      "Epoch 69: Loss: 0.5527574419975281\n",
      "Epoch 70: Loss: 0.5452187061309814\n",
      "Epoch 71: Loss: 0.5433580279350281\n",
      "Epoch 72: Loss: 0.6430413126945496\n",
      "Epoch 73: Loss: 0.5409606099128723\n",
      "Epoch 74: Loss: 0.6381754279136658\n",
      "Epoch 75: Loss: 0.6382319927215576\n",
      "Epoch 76: Loss: 0.538740336894989\n",
      "Epoch 77: Loss: 0.6383268237113953\n",
      "Epoch 78: Loss: 0.5335400104522705\n",
      "Epoch 79: Loss: 0.5367928743362427\n",
      "Epoch 80: Loss: 0.5329046249389648\n",
      "Epoch 81: Loss: 0.6386681199073792\n",
      "Epoch 82: Loss: 0.6387675404548645\n",
      "Epoch 83: Loss: 0.536731481552124\n",
      "Epoch 84: Loss: 0.6389299035072327\n",
      "Epoch 85: Loss: 0.5340709090232849\n",
      "Epoch 86: Loss: 0.5304968953132629\n",
      "Epoch 87: Loss: 0.5252686142921448\n",
      "Epoch 88: Loss: 0.6393713355064392\n",
      "Epoch 89: Loss: 0.5327227115631104\n",
      "Epoch 90: Loss: 0.5176845788955688\n",
      "Epoch 91: Loss: 0.5196082592010498\n",
      "Epoch 92: Loss: 0.5197827219963074\n",
      "Epoch 93: Loss: 0.5181814432144165\n",
      "Epoch 94: Loss: 0.5150678157806396\n",
      "Epoch 95: Loss: 0.51090407371521\n",
      "Epoch 96: Loss: 0.6415073871612549\n",
      "Epoch 97: Loss: 0.5024539828300476\n",
      "Epoch 98: Loss: 0.6422008275985718\n",
      "Epoch 99: Loss: 0.642478883266449\n",
      "Epoch 100: Loss: 0.49324074387550354\n",
      "Epoch 101: Loss: 0.733206570148468\n",
      "Epoch 102: Loss: 0.5763780474662781\n",
      "Epoch 103: Loss: 0.5047287344932556\n",
      "Epoch 104: Loss: 0.6440950036048889\n",
      "Epoch 105: Loss: 0.5438330173492432\n",
      "Epoch 106: Loss: 0.565589189529419\n",
      "Epoch 107: Loss: 0.5782840847969055\n",
      "Epoch 108: Loss: 0.6452668905258179\n",
      "Epoch 109: Loss: 0.6453683972358704\n",
      "Epoch 110: Loss: 0.6453506350517273\n",
      "Epoch 111: Loss: 0.6452255249023438\n",
      "Epoch 112: Loss: 0.5734365582466125\n",
      "Epoch 113: Loss: 0.644786536693573\n",
      "Epoch 114: Loss: 0.6444891095161438\n",
      "Epoch 115: Loss: 0.5416014790534973\n",
      "Epoch 116: Loss: 0.5291900038719177\n",
      "Epoch 117: Loss: 0.5164241194725037\n",
      "Epoch 118: Loss: 0.5055550932884216\n",
      "Epoch 119: Loss: 0.4972776174545288\n",
      "Epoch 120: Loss: 0.6434366106987\n",
      "Epoch 121: Loss: 0.6433794498443604\n",
      "Epoch 122: Loss: 0.6858460307121277\n",
      "Epoch 123: Loss: 0.6726135611534119\n",
      "Epoch 124: Loss: 0.4885697662830353\n",
      "Epoch 125: Loss: 0.6429997086524963\n",
      "Epoch 126: Loss: 0.49530255794525146\n",
      "Epoch 127: Loss: 0.4987297058105469\n",
      "Epoch 128: Loss: 0.6430104374885559\n",
      "Epoch 129: Loss: 0.5043228268623352\n",
      "Epoch 130: Loss: 0.6430864930152893\n",
      "Epoch 131: Loss: 0.5071460604667664\n",
      "Epoch 132: Loss: 0.6431291103363037\n",
      "Epoch 133: Loss: 0.6430961489677429\n",
      "Epoch 134: Loss: 0.6429813504219055\n",
      "Epoch 135: Loss: 0.506066083908081\n",
      "Epoch 136: Loss: 0.5043491721153259\n",
      "Epoch 137: Loss: 0.6427002549171448\n",
      "Epoch 138: Loss: 0.6426157355308533\n",
      "Epoch 139: Loss: 0.6424608826637268\n",
      "Epoch 140: Loss: 0.6422461271286011\n",
      "Epoch 141: Loss: 0.6419822573661804\n",
      "Epoch 142: Loss: 0.641679584980011\n",
      "Epoch 143: Loss: 0.49414965510368347\n",
      "Epoch 144: Loss: 0.49329474568367004\n",
      "Epoch 145: Loss: 0.6410602927207947\n",
      "Epoch 146: Loss: 0.4913632571697235\n",
      "Epoch 147: Loss: 0.640893280506134\n",
      "Epoch 148: Loss: 0.48967576026916504\n",
      "Epoch 149: Loss: 0.4888308346271515\n",
      "Epoch 150: Loss: 0.6409420371055603\n",
      "Epoch 151: Loss: 0.48719272017478943\n",
      "Epoch 152: Loss: 0.6411268711090088\n",
      "Epoch 153: Loss: 0.48583152890205383\n",
      "Epoch 154: Loss: 0.6438429355621338\n",
      "Epoch 155: Loss: 0.48569372296333313\n",
      "Epoch 156: Loss: 0.6416330933570862\n",
      "Epoch 157: Loss: 0.6417353749275208\n",
      "Epoch 158: Loss: 0.48733481764793396\n",
      "Epoch 159: Loss: 0.48776355385780334\n",
      "Epoch 160: Loss: 0.6420971751213074\n",
      "Epoch 161: Loss: 0.6422193646430969\n",
      "Epoch 162: Loss: 0.6422540545463562\n",
      "Epoch 163: Loss: 0.48896512389183044\n",
      "Epoch 164: Loss: 0.6422688364982605\n",
      "Epoch 165: Loss: 0.6422462463378906\n",
      "Epoch 166: Loss: 0.4893784821033478\n",
      "Epoch 167: Loss: 0.6421632766723633\n",
      "Epoch 168: Loss: 0.6421000957489014\n",
      "Epoch 169: Loss: 0.4891824424266815\n",
      "Epoch 170: Loss: 0.488857626914978\n",
      "Epoch 171: Loss: 0.4882242679595947\n",
      "Epoch 172: Loss: 0.6422215104103088\n",
      "Epoch 173: Loss: 0.48670873045921326\n",
      "Epoch 174: Loss: 0.4858790338039398\n",
      "Epoch 175: Loss: 0.6427790522575378\n",
      "Epoch 176: Loss: 0.4842001497745514\n",
      "Epoch 177: Loss: 0.6432210803031921\n",
      "Epoch 178: Loss: 0.4827742874622345\n",
      "Epoch 179: Loss: 0.48206987977027893\n",
      "Epoch 180: Loss: 0.6439879536628723\n",
      "Epoch 181: Loss: 0.4807192385196686\n",
      "Epoch 182: Loss: 0.6445348858833313\n",
      "Epoch 183: Loss: 0.6447216868400574\n",
      "Epoch 184: Loss: 0.4793640077114105\n",
      "Epoch 185: Loss: 0.650518000125885\n",
      "Epoch 186: Loss: 0.6450181007385254\n",
      "Epoch 187: Loss: 0.6449599266052246\n",
      "Epoch 188: Loss: 0.6447929739952087\n",
      "Epoch 189: Loss: 0.6445322036743164\n",
      "Epoch 190: Loss: 0.644193172454834\n",
      "Epoch 191: Loss: 0.643791675567627\n",
      "Epoch 192: Loss: 0.48719310760498047\n",
      "Epoch 193: Loss: 0.4883817732334137\n",
      "Epoch 194: Loss: 0.6429070830345154\n",
      "Epoch 195: Loss: 0.48979008197784424\n",
      "Epoch 196: Loss: 0.6425960659980774\n",
      "Epoch 197: Loss: 0.4901922941207886\n",
      "Epoch 198: Loss: 0.48989352583885193\n",
      "Epoch 199: Loss: 0.4891466200351715\n",
      "Epoch 200: Loss: 0.48807016015052795\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import networkx as nx\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch_geometric.nn import GCNConv  # Change this to your preferred GNN layer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_edge(0, 1, text=\"Hello from node 0 to node 1\")\n",
    "G.add_edge(1, 2, text=\"Message from node 1 to node 2\")\n",
    "G.add_nodes_from([\n",
    "    (0, {\"text\": \"Node 0's des.\"}),\n",
    "    (1, {\"text\": \"Node 1's des\"}),\n",
    "    (2, {\"text\": \"Node 2's des.\"}),\n",
    "])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "def encode_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "\n",
    "def convert_nx_to_pyg(G):\n",
    "    texts = [G.nodes[node]['text'] for node in G.nodes()]\n",
    "    x = torch.cat([encode_text(text) for text in texts], dim=0)\n",
    "    \n",
    "    edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    data = Data(x=x, edge_index=edge_index)\n",
    "    return data\n",
    "\n",
    "data = convert_nx_to_pyg(G)\n",
    "\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(data.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GNN(hidden_channels=16)\n",
    "\n",
    "labels = torch.tensor([0, 1, 0], dtype=torch.long)\n",
    "data.y = labels\n",
    "\n",
    "data.train_mask = torch.tensor([True, True, True], dtype=torch.bool)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch+1}: Loss: {loss.item()}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN + Transformer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0857,  0.1219]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.nn import Linear, Dropout\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomGNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(CustomGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GNNTransformerModel(torch.nn.Module):\n",
    "    def __init__(self, gnn_hidden_dim, transformer_model_name, num_labels):\n",
    "        super(GNNTransformerModel, self).__init__()\n",
    "        self.gnn = CustomGNN(input_dim=768, hidden_dim=gnn_hidden_dim)\n",
    "        self.transformer = AutoModelForSequenceClassification.from_pretrained(transformer_model_name, num_labels=num_labels, output_hidden_states=True)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(transformer_model_name)\n",
    "        self.dropout = Dropout(0.1)\n",
    "        self.classifier = Linear(gnn_hidden_dim + self.transformer.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, text, gnn_data):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        transformer_outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        transformer_hidden_states = transformer_outputs.hidden_states[-1]\n",
    "        pooled_output = transformer_hidden_states[:, 0]\n",
    "\n",
    "\n",
    "        gnn_output = self.gnn(gnn_data[\"x\"], gnn_data[\"edge_index\"])\n",
    "        gnn_output = gnn_output.mean(dim=0, keepdim=True)\n",
    "\n",
    "        combined_output = torch.cat((pooled_output, gnn_output), dim=1)\n",
    "        combined_output = self.dropout(combined_output)\n",
    "        logits = self.classifier(combined_output)\n",
    "        return logits\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "gnn_transformer_model = GNNTransformerModel(gnn_hidden_dim=128, transformer_model_name=model_name, num_labels=2)\n",
    "\n",
    "text = \"Example text\"\n",
    "gnn_data = {\n",
    "    \"x\": torch.randn(10, 768), \n",
    "    \"edge_index\": torch.tensor([[0, 1], [1, 2]], dtype=torch.long) \n",
    "}\n",
    "\n",
    "logits = gnn_transformer_model(text, gnn_data)\n",
    "print(logits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
